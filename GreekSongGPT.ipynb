{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing some useful libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cores = 6\n",
    "heads = 8\n",
    "head_sze = 64\n",
    "\n",
    "embedding_size = 512\n",
    "context = 256\n",
    "p = 0.3\n",
    "\n",
    "learning_rate = 1e-6\n",
    "N_iterations = 1500\n",
    "batches = 64\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reading and storing the txt file in one string variable\n",
    "\n",
    "with open('songs.txt', 'r', encoding='utf-8') as f:\n",
    "    songs = f.read()\n",
    "\n",
    "# mapping characters to integers\n",
    "\n",
    "chars = sorted(list(set(''.join(songs))))\n",
    "total_chr = len(chars)\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# convert string to list of integers or the inverse\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# encoding the whole string to tensor of integers\n",
    "\n",
    "text = torch.tensor(encode(songs), dtype = torch.long)\n",
    "\n",
    "# splitting the data in training and testing data (4:1 ratio)\n",
    "\n",
    "train_data = text[:int(0.8*len(text))]\n",
    "test_data = text[int(0.8*len(text)):]"
   ],
   "id": "a9499830348aa11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# I will build a single head of attention\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_sze):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embedding_size, head_sze, bias=False)  # projecting embedding to head size\n",
    "        self.query = nn.Linear(embedding_size, head_sze, bias=False)  # projecting embedding to head size\n",
    "        self.value = nn.Linear(embedding_size, head_sze, bias=False)  # projecting embedding to head size\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context, context)))\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, num):\n",
    "        B, T, C = num.shape  # B:batches T:number of characters C:channels(every character of every batch has an embedding table)\n",
    "\n",
    "        keys = self.key(num)  # is what i have (B,T,head_sze)\n",
    "        queries = self.query(num)  # is what i need (B,T,head_sze)\n",
    "        values = self.value(num)  # is what i get (B,T,head_sze)\n",
    "\n",
    "        tmp = queries @ keys.transpose(-2, -1) * keys.shape[-1] ** -0.5\n",
    "\n",
    "        tmp = tmp.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        tmp = F.softmax(tmp, dim=-1)\n",
    "\n",
    "        tmp = self.dropout(tmp)\n",
    "\n",
    "        out = tmp @ values\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# now I will produce multiple heads of attention\n",
    "\n",
    "class Multiple_Heads_of_Attention(nn.Module):\n",
    "    def __init__(self, heads, head_sze):\n",
    "        super().__init__()\n",
    "        self.list_of_heads = nn.ModuleList(Head(head_sze) for _ in range(heads))\n",
    "        self.proj = nn.Linear(head_sze * heads, embedding_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, num):\n",
    "        ans = torch.cat([hd(num) for hd in self.list_of_heads],dim=-1)\n",
    "        ans = self.dropout(self.proj(ans))\n",
    "        return ans\n",
    "\n",
    "\n",
    "# now linearity with non-linearity(RELU) Feed Forward\n",
    "\n",
    "class FF(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "\n",
    "    def forward(self, num):\n",
    "        return self.network(num)\n",
    "\n",
    "\n",
    "class Core(nn.Module):\n",
    "    def __init__(self, embedding_size, heads):\n",
    "        super().__init__()\n",
    "        self.mha = Multiple_Heads_of_Attention(heads, embedding_size // heads)\n",
    "        self.ff = FF(embedding_size)\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, num):\n",
    "        num = num + self.mha(self.ln1(num))  # residual connection\n",
    "        num = num + self.ff(self.ln2(num))  # residual connection\n",
    "        return num\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.chr_emb = nn.Embedding(total_chr, embedding_size)\n",
    "        self.pos_emb = nn.Embedding(context, embedding_size)\n",
    "        self.cores = nn.Sequential(*[Core(embedding_size, heads = heads) for _ in range(cores)])\n",
    "        self.final_ln = nn.LayerNorm(embedding_size)\n",
    "        self.final_proj = nn.Linear(embedding_size, total_chr)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, t=None):\n",
    "        B, T = index.shape\n",
    "        emb_char = self.chr_emb(index)\n",
    "        emb_pos = self.pos_emb(torch.arange(T, device=device))\n",
    "        num = emb_char + emb_pos\n",
    "        num = self.cores(num)\n",
    "        num = self.final_ln(num)\n",
    "        logits = self.final_proj(num)\n",
    "\n",
    "        if t is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            t = t.view(B * T)\n",
    "            loss = F.cross_entropy(logits, t)\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, ans, max_chars):\n",
    "\n",
    "        for _ in range(max_chars):\n",
    "            ans_croped = ans[:, -context:]\n",
    "            logits, loss = self(ans_croped)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            nxt = torch.multinomial(probabilities, num_samples = 1)\n",
    "            ans = torch.cat((ans, nxt), dim = 1)\n",
    "        return ans\n",
    "\n",
    "\n"
   ],
   "id": "1794c92bd94e1243"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Transformer()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters()), 'parameters')\n"
   ],
   "id": "4adf3d805c061dfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# using adam optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ],
   "id": "42fab5291eee4ffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def to build batches in training data\n",
    "\n",
    "def batch_in_training():\n",
    "  start = torch.randint(len(train_data) - context , (batches,))\n",
    "  X = torch.stack([train_data[srt:srt+context] for srt in start])\n",
    "  Y = torch.stack([train_data[srt+1:srt+context+1] for srt in start])\n",
    "  X = X.to(device)\n",
    "  Y = Y.to(device)\n",
    "  return X,Y\n",
    "\n",
    "# def to build batches in test data\n",
    "\n",
    "def batch_in_test():\n",
    "  start = torch.randint(len(test_data) - context , (batches,))\n",
    "  X = torch.stack([test_data[srt:srt+context] for srt in start])\n",
    "  Y = torch.stack([test_data[srt+1:srt+context+1] for srt in start])\n",
    "  X = X.to(device)\n",
    "  Y = Y.to(device)\n",
    "  return X,Y\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "  out = {}\n",
    "  model.eval()\n",
    "  losses_train = torch.zeros(40)\n",
    "  losses_test = torch.zeros(40)\n",
    "  for i in range(40):\n",
    "    xb_train, yb_train = batch_in_training()\n",
    "    logits_train, loss_train = model(xb_train,yb_train)\n",
    "    losses_train[i] = loss_train.item()\n",
    "    xb_test, yb_test = batch_in_test()\n",
    "    logits_test, loss_test = model(xb_test,yb_test)\n",
    "    losses_test[i] = loss_test.item()\n",
    "  out['train'] = losses_train.mean()\n",
    "  out['test'] = losses_test.mean()\n",
    "  model.train()\n",
    "  return out"
   ],
   "id": "ed5d1e1dc1815b04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tmp = torch.zeros((1, 7), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(tmp, max_chars=2000)[0].tolist()))"
   ],
   "id": "a2f4cc6ca2f0c250"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
